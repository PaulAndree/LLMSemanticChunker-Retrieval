{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13834d78-1bb1-44fe-8302-73872cfd45b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/embeddings/embeddings/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:00<00:00, 30.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import re, os, time, argparse\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "#Embedding model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-base\")\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "#LLM model\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f59a66c-81d6-4beb-96cf-4b7a006f0187",
   "metadata": {},
   "source": [
    "## Limpieza del documento Mark Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b6ad24-838f-4a31-8a17-34a716fd6e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Demostración de Procesamiento de Documentos en Español con IA\\n\\nimmediate  \\n\\nLa inteligencia artificial (IA) es un campo multidisciplinario de la informática que busca desarrollar sistemas capaces de realizar tareas que normalmente requieren inteligencia humana, como el reconocimiento de patrones, la toma de decisiones, el aprendizaje \\\\(\\\\mathbf{y}\\\\) la resolución de problemas. La IA se divide en varias subáreas, como el aprendizaje automático (machine learning), la visión por computadora, el procesamiento del lenguaje natural (NLP) \\\\(\\\\mathbf{y}\\\\) la robótica. Estos sistemas pueden ser entrenados para mejorar su desempeño con el tiempo mediante el uso de grandes cantidades de datos y algoritmos especializados.  \\n\\n # Introducción\\n\\nEl concepto de inteligencia artificial posee diversas perspectivas de acuerdo con el contexto en el que se usa. Sin embargo, una definición clásica y de amplia aceptación por la comunidad científica y tecnológica es la proporcionada por los profesores Peter Norvig y Stuart Russell, quienes definen a la inteligencia artificial como el diseño y la construcción de agentes inteligentes que reciben percepciones del entorno y emprenden acciones que afectan ese entorno. Dichos agentes pueden definirse con las siguientes fórmulas:  \\n\\n \\\\begin{equation}\\nf(x)=f(a)+f^{\\\\prime}(a)(x-a)+{\\\\frac{f^{\\\\prime\\\\prime}(a)}{2!}}(x-a)^{2}\\n\\\\end{equation}   \\n\\n \\\\begin{equation}\\ny(x)=\\\\int_{x_{0}}^{x}\\\\left(\\\\frac{y_{1}(t)y_{2}(x)-y_{1}(x)y_{2}(t)}{W(t)}\\\\right)f(t)d t\\n\\\\end{equation}   \\n\\n # Resultados\\n\\nEl análisis de los datos mostró una mejora significativa en los indicadores de salud cardiovascular de los participantes después de la intervención. Al comparar los valores iniciales y finales, se observó una reducción media en la presión arterial sistólica de \\\\(12.3\\\\mathrm{mmHg}\\\\) y en la presión arterial diastólica de \\\\(8.7\\\\mathrm{mmHg}\\\\) . Además, los niveles de colesterol LDL disminuyeron un promedio de \\\\(15\\\\mathrm{mg/dL}\\\\) , mientras que el colesterol HDL aumentó en \\\\(5~\\\\mathrm{mg/dL}\\\\) . Estos cambios fueron modelados usando la siguiente fórmula.  \\n\\n \\\\begin{equation}\\nx={\\\\frac{-b\\\\pm{\\\\sqrt{b^{2}-4a c}}}{2a}}+{\\\\frac{\\\\sqrt{\\\\left({\\\\frac{-b+{\\\\sqrt{b^{2}-4a c}}}{2a}}\\\\right)^{2}+4}}{2}}\\n\\\\end{equation}   \\n\\n\\n\\nEn resumen, los resultados sugieren que el programa de ejercicio físico implementado tuvo un impacto positivo en la reducción de los factores de riesgo cardiovascular y en la mejora de la capacidad física de los participantes.  \\n\\nEl chunking semántico es una técnica de procesamiento del lenguaje natural (PLN) que divide un texto en fragmentos (chunks) con significado independiente, como frases o cláusulas coherentes. A diferencia del chunking sintáctico, que separa palabras según su estructura gramatical (sujeto, verbo, objeto, etc.), el chunking semántico puede observarse en la Figura 2.  \\n\\n # Conclusiones\\n\\nEn este trabajo se ha desarrollado y evaluado el Sistema Generador y Evaluador de Cuestionarios (SGEC), un sistema basado en el modelo de lenguaje preentrenado Llama3-8B para la generación personalizada de preguntas y evaluación automática de respuestas.  \\n\\n\\nEn primer lugar, el principal logro de este trabajo radica en que el sistema SGEC es capaz de generar preguntas que se alineen adecuadamente con las dimensiones cognitivas y de conocimiento de la taxonomía de Bloom mediante el uso de prompts y técnicas de aprendizaje contextual, concretamente few-shot. Esto permitió en última instancia controlar el nivel de dificultad deseado (fácil, intermedio y difícil), tal y como lo confirma dos encuestas realizadas en este trabajo.  \\n\\n\\n # Bibliografía\\n\\n\\nEste sistema generador de preguntas controla la dificultad de las preguntas en tres niveles (fácil, intermedio y difícil). La aplicación está diseñada para permitir al estudiante seleccionar el número deseado de preguntas, la dificultad y el tipo de preguntas en las que quiera examinarse (preguntas abiertas y de opción múltiple) pudiendo también seleccionar el porcentaje de la cantidad de ca- da tipo de pregunta en un cuestionario. Además de la generación de cuestionarios, se propone la evaluación automática de respuestas para las preguntas abiertas (Anexo 3). De esta manera logramos implementar un aprendizaje adaptativo completo que se ajusta a la situación en el proceso de aprendizaje de cada estudiante.  '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def md_to_latex(md_text):\n",
    "\n",
    "    def convert_heading(match):\n",
    "        title_text = match.group(2).strip()\n",
    "        # Match numbers at the start of the heading (including multiple dots like \"0.1.1\")\n",
    "        number_match = re.match(r'^([\\d.]+)\\s+', title_text)\n",
    "        if number_match:\n",
    "            number = number_match.group(1)  # Extract matched number\n",
    "            title_text = title_text[len(number_match.group(0)):]  # Remove number + space\n",
    "            if '.' in number:  # If it contains a dot (1.2 or 0.1.1) -> subsection = ##\n",
    "                return rf' ## {title_text}'\n",
    "            else:  # Otherwise (integer like \"1\", \"2\") -> section = #\n",
    "                return rf' # {title_text}'\n",
    "        return rf' # {title_text}'  # Default case\n",
    "\n",
    "    md_text = re.sub(r'^(#)\\s+(.+)', convert_heading, md_text, flags=re.MULTILINE)   \n",
    "    md_text = re.sub(r'\\s+}', '}', md_text)\n",
    "\n",
    "    \n",
    "    # regex pattern for removin figures\n",
    "    pattern = r'!\\[\\]\\((.*?)\\)\\s*((?:Figura|Figure|Fig.)\\s+\\d+(?:\\.\\d+)?\\s*.*?)\\n'\n",
    "    \n",
    "    md_text = re.sub(pattern, '', md_text)\n",
    "    \n",
    "    # Convert Block Formulas with Captions\n",
    "    def convert_equation(match):\n",
    "        equation = match.group(1).strip()\n",
    "        return f\" \\\\begin{{equation}}\\n{equation}\\n\\\\end{{equation}} \"\n",
    "\n",
    "    md_text = re.sub(r'\\$\\$(.*?)\\$\\$', convert_equation, md_text, flags=re.DOTALL)\n",
    "    \n",
    "    # Convert Inline Formulas\n",
    "    md_text = re.sub(r'\\$(.*?)\\$', r'\\\\(\\1\\\\)', md_text)  # Inline math: $x^2$ -> \\(x^2\\)\n",
    "    \n",
    "        \n",
    "    def convert_table(match):\n",
    "        table_html = match.group(2)  # Captured table HTML\n",
    "        caption_before = re.search(r'^\\s*(Table [IVXLCDM\\d]+.*?)$', match.group(1), re.MULTILINE) if match.group(1) else None\n",
    "        caption_after = re.search(r'^\\s*(Table [IVXLCDM\\d]+.*?)$', match.group(3), re.MULTILINE) if match.group(3) else None\n",
    "        caption = caption_before.group(1) if caption_before else (caption_after.group(1) if caption_after else \"Table caption\")\n",
    "        # Extract table rows\n",
    "        rows = re.findall(r'<tr>(.*?)</tr>', table_html, re.DOTALL)\n",
    "        if not rows:\n",
    "            return \"\"  # If no rows are found, return empty string (invalid table)\n",
    "\n",
    "        # Detect number of columns\n",
    "        first_row = re.findall(r'<t[dh]>(.*?)</t[dh]>', rows[0])\n",
    "        num_columns = len(first_row) if first_row else 1  # Default to 1 if no columns detected\n",
    "\n",
    "        # Start LaTeX table\n",
    "        latex_table = \" \\\\begin{table}[h]\\n\\\\centering\\n\"\n",
    "        latex_table += \"\\\\begin{tabular}{\" + \"|c\" * num_columns + \"|}\\n\\\\hline\\n\"\n",
    "        for row in rows:\n",
    "            cells = re.findall(r'<t[dh]>(.*?)</t[dh]>', row)\n",
    "            latex_table += \" & \".join(cells) + \" \\\\\\\\\\n\\\\hline\\n\"\n",
    "\n",
    "        latex_table += f\"\\\\end{{tabular}}\\n\\\\caption{{{caption}}}\\n\\\\end{{table}}\\n \"\n",
    "        return latex_table\n",
    "\n",
    "    table_pattern = re.compile(\n",
    "        r'\\s*((?:Table|Tabla)\\s+[IVXLCDM\\d]+[^.\\n]*[:.]?)?'  # Accepts \"Table II:\", \"Tabla 2.\"\n",
    "        r'\\s*\\n*'  # Allow one or more newlines between caption and table\n",
    "        r'\\s*<html><body><table>(.*?)</table></body></html>'  # detect the HTML table content\n",
    "        r'\\s*((?:Table|Tabla)\\s+[IVXLCDM\\d]+[^.\\n]*[:.]?)?',  # Optional caption after\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Replace Markdown tables with LaTeX\n",
    "    md_text = re.sub(table_pattern, convert_table, md_text)\n",
    "    \n",
    "    #remove citations\n",
    "    md_text = re.sub(r'\\[\\d+\\][,\\-]?', '', md_text)\n",
    "    \n",
    "    # Define regex pattern to match the section title in different languages\n",
    "    pattern = r'(\\\\section\\s*\\{(REFERENCES|REFERENCIAS|BIBLIOGRAPHY|BIBLIOGRAFIA)\\s*\\}).*'\n",
    "\n",
    "    # Remove everything after the matched section title\n",
    "    md_text = re.sub(pattern, r'\\1', md_text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    \n",
    "    return md_text \n",
    "\n",
    "with open(\"05_SPA.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    markdown_content = f.read()\n",
    "    \n",
    "latex_content = md_to_latex(markdown_content)\n",
    "latex_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30b9ef-01e3-4d37-9550-2542d47c6b8c",
   "metadata": {},
   "source": [
    "## CHUNKING RECURSIVE + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6957050-d381-4429-850e-f6487f8e75bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Demostración de Procesamiento de Documentos en Español con IA\\nimmediate  \\n\\nLa inteligencia artificial (IA) es un campo multidisciplinario de la informática que busca desarrollar sistemas capaces de realizar tareas que normalmente requieren inteligencia humana, como el reconocimiento de patrones, la toma de decisiones, el aprendizaje \\\\(\\\\mathbf{y}\\\\) la resolución de problemas. La IA se divide en varias subáreas, como el aprendizaje automático (machine learning), la visión por computadora, el procesamiento del lenguaje natural (NLP) \\\\(\\\\mathbf{y}\\\\) la robótica. Estos sistemas pueden ser entrenados para mejorar su desempeño con el tiempo mediante el uso de grandes cantidades de datos y algoritmos especializados.',\n",
       " '# Introducción\\nEl concepto de inteligencia artificial posee diversas perspectivas de acuerdo con el contexto en el que se usa. Sin embargo, una definición clásica y de amplia aceptación por la comunidad científica y tecnológica es la proporcionada por los profesores Peter Norvig y Stuart Russell, quienes definen a la inteligencia artificial como el diseño y la construcción de agentes inteligentes que reciben percepciones del entorno y emprenden acciones que afectan ese entorno. Dichos agentes pueden definirse con las siguientes fórmulas:  \\n\\n \\\\begin{equation}\\nf(x)=f(a)+f^{\\\\prime}(a)(x-a)+{\\\\frac{f^{\\\\prime\\\\prime}(a)}{2!}}(x-a)^{2}\\n\\\\end{equation}   \\n\\n \\\\begin{equation}\\ny(x)=\\\\int_{x_{0}}^{x}\\\\left(\\\\frac{y_{1}(t)y_{2}(x)-y_{1}(x)y_{2}(t)}{W(t)}\\\\right)f(t)d t\\n\\\\end{equation}',\n",
       " '# Resultados\\nEl análisis de los datos mostró una mejora significativa en los indicadores de salud cardiovascular de los participantes después de la intervención. Al comparar los valores iniciales y finales, se observó una reducción media en la presión arterial sistólica de \\\\(12.3\\\\mathrm{mmHg}\\\\) y en la presión arterial diastólica de \\\\(8.7\\\\mathrm{mmHg}\\\\) . Además, los niveles de colesterol LDL disminuyeron un promedio de \\\\(15\\\\mathrm{mg/dL}\\\\) , mientras que el colesterol HDL aumentó en \\\\(5~\\\\mathrm{mg/dL}\\\\) . Estos cambios fueron modelados usando la siguiente fórmula.  \\n\\n \\\\begin{equation}\\nx={\\\\frac{-b\\\\pm{\\\\sqrt{b^{2}-4a c}}}{2a}}+{\\\\frac{\\\\sqrt{\\\\left({\\\\frac{-b+{\\\\sqrt{b^{2}-4a c}}}{2a}}\\\\right)^{2}+4}}{2}}\\n\\\\end{equation}   \\n\\n\\n\\nEn resumen, los resultados sugieren que el programa de ejercicio físico implementado tuvo un impacto positivo en la reducción de los factores de riesgo cardiovascular y en la mejora de la capacidad física de los participantes.  \\n\\nEl chunking semántico es una técnica de procesamiento del lenguaje natural (PLN) que divide un texto en fragmentos (chunks) con significado independiente, como frases o cláusulas coherentes. A diferencia del chunking sintáctico, que separa palabras según su estructura gramatical (sujeto, verbo, objeto, etc.), el chunking semántico puede observarse en la Figura 2.',\n",
       " '# Conclusiones\\nEn este trabajo se ha desarrollado y evaluado el Sistema Generador y Evaluador de Cuestionarios (SGEC), un sistema basado en el modelo de lenguaje preentrenado Llama3-8B para la generación personalizada de preguntas y evaluación automática de respuestas.  \\n\\n\\nEn primer lugar, el principal logro de este trabajo radica en que el sistema SGEC es capaz de generar preguntas que se alineen adecuadamente con las dimensiones cognitivas y de conocimiento de la taxonomía de Bloom mediante el uso de prompts y técnicas de aprendizaje contextual, concretamente few-shot. Esto permitió en última instancia controlar el nivel de dificultad deseado (fácil, intermedio y difícil), tal y como lo confirma dos encuestas realizadas en este trabajo.',\n",
       " '# Bibliografía\\nEste sistema generador de preguntas controla la dificultad de las preguntas en tres niveles (fácil, intermedio y difícil). La aplicación está diseñada para permitir al estudiante seleccionar el número deseado de preguntas, la dificultad y el tipo de preguntas en las que quiera examinarse (preguntas abiertas y de opción múltiple) pudiendo también seleccionar el porcentaje de la cantidad de ca- da tipo de pregunta en un cuestionario. Además de la generación de cuestionarios, se propone la evaluación automática de respuestas para las preguntas abiertas (Anexo 3). De esta manera logramos implementar un aprendizaje adaptativo completo que se ajusta a la situación en el proceso de aprendizaje de cada estudiante.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = []\n",
    "\n",
    "def count_tokens(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return len(tokens)\n",
    "    \n",
    "def llm_chunk(paragraph):\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    Eres un asistente experto en segmentación semántica, \n",
    "    tu tarea es dividir cualquier texto en segmentos coherentes y con significado semántico.<|start_header_id|>user<|end_header_id|>\n",
    "    Por favor, divide el siguiente texto en segmentos con significado semántico. \n",
    "    Cada segmento debe contener oraciones completas y debe tener entre 380 o 400 tokens como máximo. En caso de encontrar formulas matematicas en formato latex, debes mantenerlas obligatoriamente.\n",
    "    Al inicio de cada segmento debes añadir la etiqueta <chunk_begining> y al final de cada fragmento, añade <chunk_end>.\n",
    "    Texto:\n",
    "    \n",
    "    \\\"\\\"\\\"{paragraph}\\\"\\\"\\\"\n",
    "    \n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "    \n",
    "    generator = pipeline(\"text-generation\", model=llm_model, tokenizer=llm_tokenizer)\n",
    "    output = generator(\n",
    "        prompt,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=400,\n",
    "        return_full_text=False\n",
    "    )\n",
    "    answer = output[0]['generated_text']\n",
    "    pattern = r\"<chunk_begining>\\s*(.*?)\\s*<chunk_end>\"\n",
    "    chunks = re.findall(pattern, answer, re.DOTALL)\n",
    "    \n",
    "    return chunks\n",
    "    \n",
    "def split_markdown_sections(markdown_text: str):\n",
    "    \"\"\"\n",
    "    Splits markdown into sections every time there's a title (#) or subtitle (##),\n",
    "    BUT merges a subtitle immediately following a title as one section.\n",
    "    Returns list of dicts: {\"section_title\": ..., \"content\": ...}\n",
    "    \"\"\"\n",
    "    # Find all headings and their positions\n",
    "    #heading_pattern = re.compile(r'^(#{1,2}) (.+)$', re.MULTILINE)\n",
    "    heading_pattern = re.compile(r'^\\s*(#{1,2})\\s+(.+)$', re.MULTILINE)\n",
    "    matches = list(heading_pattern.finditer(markdown_text))\n",
    "\n",
    "    sections = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(matches):\n",
    "        match = matches[i]\n",
    "        level = len(match.group(1))\n",
    "        heading_text = match.group(2).strip()\n",
    "        start = match.end()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(markdown_text)\n",
    "\n",
    "        section_title = f\"{match.group(1)} {heading_text}\"\n",
    "        section_content_start = start\n",
    "        section_content_end = end\n",
    "        i += 1\n",
    "\n",
    "        # If current is Title (#) and next is Subtitle (##) — merge\n",
    "        if level == 1 and i < len(matches):\n",
    "            next_match = matches[i]\n",
    "            next_level = len(next_match.group(1))\n",
    "            if next_level == 2:\n",
    "                # Merge next subtitle into current title\n",
    "                next_heading_text = next_match.group(2).strip()\n",
    "                section_title += f\"\\n{next_match.group(1)} {next_heading_text}\"\n",
    "                section_content_start = next_match.end()\n",
    "                end = matches[i + 1].start() if i + 1 < len(matches) else len(markdown_text)\n",
    "                section_content_end = end\n",
    "                i += 1  # Skip the subtitle match\n",
    "\n",
    "        content = markdown_text[section_content_start:section_content_end].strip()\n",
    "        sections.append({\n",
    "            \"section_title\": section_title,\n",
    "            \"content\": content\n",
    "        })\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "def recursive_chunk(text: str, max_tokens: int = MAX_TOKENS):\n",
    "    \"\"\"\n",
    "    Recursively splits text into chunks no larger than max_tokens.\n",
    "    First case scenario is when the paragraph´s token lenght is the same or smaller than max_tokens\n",
    "    Second case scenario is when the paragraph´s token lenght is bigger than max_tokens. In that case we split by /n/n\n",
    "    \"\"\"\n",
    "            \n",
    "    chunks = []\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    if len(tokens) < max_tokens:\n",
    "        chunks.append(text)\n",
    "\n",
    "    else:\n",
    "        section = text.split('\\n\\n')\n",
    "        for paragraph in section:\n",
    "            chunk_text =  llm_chunk(paragraph)\n",
    "            chunks.append(chunk_text)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def retrieve_top_k(model, id_map, metadata, query, k=5):\n",
    "    # Function to retrieve top 3 contexts\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(query_embedding)  # Normalize query for cosine similarity\n",
    "    start_time = time.time()\n",
    "    distances, indices = id_map.search(query_embedding, k)\n",
    "    end_time = time.time()\n",
    "    retrieval_time = end_time - start_time\n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx != -1:\n",
    "            meta = metadata[idx]\n",
    "            results.append({\n",
    "                #\"full_paragraph\": meta[\"full_paragraph\"],\n",
    "                \"chunk_text\": meta[\"chunk_text\"],\n",
    "                \"distance\": float(distances[0][i]) \n",
    "            })\n",
    "    \n",
    "    return results, retrieval_time\n",
    "    \n",
    "def chunk_markdown(markdown_text: str, max_tokens: int = MAX_TOKENS):\n",
    "    \"\"\"\n",
    "        Splits markdown into chunks\n",
    "    \"\"\"\n",
    "    index_start_time = time.time()\n",
    "    sections = split_markdown_sections(markdown_text)   \n",
    "    all_chunks = []\n",
    "    \n",
    "    for section in sections:\n",
    "        title = section['section_title']\n",
    "        content = section['content']\n",
    "        \n",
    "        combined_text = f\"{title}\\n{content}\".strip()\n",
    "        \n",
    "        llm_chunks = recursive_chunk(combined_text)\n",
    "\n",
    "        for i in llm_chunks:\n",
    "            all_chunks.append(i)\n",
    "            metadata.append({\n",
    "                \"full_paragraph\": combined_text,\n",
    "                \"chunk_text\": i\n",
    "            })\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "chunks = chunk_markdown(latex_content)\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "025c3660-aa12-4ab8-b4a6-1f9ebb6a3b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n",
      "Creating FAISS index...\n",
      "Storing embeddings...\n",
      "Saving FAISS index...\n",
      "Indexing complete in 0.0706 seconds! Stored FAISS index and metadata.\n"
     ]
    }
   ],
   "source": [
    "#EMBEDDINGS FOR RETRIEVAL\n",
    "\n",
    "index_start_time = time.time()\n",
    "print(\"Computing embeddings...\")\n",
    "embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "faiss.normalize_L2(embeddings)  # Normalize embeddings for cosine similarity\n",
    "\n",
    "print(\"Creating FAISS index...\")\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # Use Inner Product for Cosine Similarity\n",
    "id_map = faiss.IndexIDMap(index)\n",
    "\n",
    "print(\"Storing embeddings...\") # Assign unique IDs and store embeddings\n",
    "ids = np.arange(len(embeddings))\n",
    "id_map.add_with_ids(embeddings, ids)\n",
    "\n",
    "print(\"Saving FAISS index...\")\n",
    "faiss.write_index(id_map, \"rag_faiss.index\")\n",
    "\n",
    "index_end_time = time.time()\n",
    "indexing_time = index_end_time - index_start_time\n",
    "print(f\"Indexing complete in {indexing_time:.4f} seconds! Stored FAISS index and metadata.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "708acdb7-bc36-4426-b5b2-5d63e99251bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Qué es la inteligencia artificial?',\n",
       "  'contexts': [{'chunk_text': '# Demostración de Procesamiento de Documentos en Español con IA\\nimmediate  \\n\\nLa inteligencia artificial (IA) es un campo multidisciplinario de la informática que busca desarrollar sistemas capaces de realizar tareas que normalmente requieren inteligencia humana, como el reconocimiento de patrones, la toma de decisiones, el aprendizaje \\\\(\\\\mathbf{y}\\\\) la resolución de problemas. La IA se divide en varias subáreas, como el aprendizaje automático (machine learning), la visión por computadora, el procesamiento del lenguaje natural (NLP) \\\\(\\\\mathbf{y}\\\\) la robótica. Estos sistemas pueden ser entrenados para mejorar su desempeño con el tiempo mediante el uso de grandes cantidades de datos y algoritmos especializados.',\n",
       "    'distance': 0.8579941987991333},\n",
       "   {'chunk_text': '# Introducción\\nEl concepto de inteligencia artificial posee diversas perspectivas de acuerdo con el contexto en el que se usa. Sin embargo, una definición clásica y de amplia aceptación por la comunidad científica y tecnológica es la proporcionada por los profesores Peter Norvig y Stuart Russell, quienes definen a la inteligencia artificial como el diseño y la construcción de agentes inteligentes que reciben percepciones del entorno y emprenden acciones que afectan ese entorno. Dichos agentes pueden definirse con las siguientes fórmulas:  \\n\\n \\\\begin{equation}\\nf(x)=f(a)+f^{\\\\prime}(a)(x-a)+{\\\\frac{f^{\\\\prime\\\\prime}(a)}{2!}}(x-a)^{2}\\n\\\\end{equation}   \\n\\n \\\\begin{equation}\\ny(x)=\\\\int_{x_{0}}^{x}\\\\left(\\\\frac{y_{1}(t)y_{2}(x)-y_{1}(x)y_{2}(t)}{W(t)}\\\\right)f(t)d t\\n\\\\end{equation}',\n",
       "    'distance': 0.8340086936950684},\n",
       "   {'chunk_text': '# Bibliografía\\nEste sistema generador de preguntas controla la dificultad de las preguntas en tres niveles (fácil, intermedio y difícil). La aplicación está diseñada para permitir al estudiante seleccionar el número deseado de preguntas, la dificultad y el tipo de preguntas en las que quiera examinarse (preguntas abiertas y de opción múltiple) pudiendo también seleccionar el porcentaje de la cantidad de ca- da tipo de pregunta en un cuestionario. Además de la generación de cuestionarios, se propone la evaluación automática de respuestas para las preguntas abiertas (Anexo 3). De esta manera logramos implementar un aprendizaje adaptativo completo que se ajusta a la situación en el proceso de aprendizaje de cada estudiante.',\n",
       "    'distance': 0.7805434465408325},\n",
       "   {'chunk_text': '# Conclusiones\\nEn este trabajo se ha desarrollado y evaluado el Sistema Generador y Evaluador de Cuestionarios (SGEC), un sistema basado en el modelo de lenguaje preentrenado Llama3-8B para la generación personalizada de preguntas y evaluación automática de respuestas.  \\n\\n\\nEn primer lugar, el principal logro de este trabajo radica en que el sistema SGEC es capaz de generar preguntas que se alineen adecuadamente con las dimensiones cognitivas y de conocimiento de la taxonomía de Bloom mediante el uso de prompts y técnicas de aprendizaje contextual, concretamente few-shot. Esto permitió en última instancia controlar el nivel de dificultad deseado (fácil, intermedio y difícil), tal y como lo confirma dos encuestas realizadas en este trabajo.',\n",
       "    'distance': 0.761337161064148},\n",
       "   {'chunk_text': '# Resultados\\nEl análisis de los datos mostró una mejora significativa en los indicadores de salud cardiovascular de los participantes después de la intervención. Al comparar los valores iniciales y finales, se observó una reducción media en la presión arterial sistólica de \\\\(12.3\\\\mathrm{mmHg}\\\\) y en la presión arterial diastólica de \\\\(8.7\\\\mathrm{mmHg}\\\\) . Además, los niveles de colesterol LDL disminuyeron un promedio de \\\\(15\\\\mathrm{mg/dL}\\\\) , mientras que el colesterol HDL aumentó en \\\\(5~\\\\mathrm{mg/dL}\\\\) . Estos cambios fueron modelados usando la siguiente fórmula.  \\n\\n \\\\begin{equation}\\nx={\\\\frac{-b\\\\pm{\\\\sqrt{b^{2}-4a c}}}{2a}}+{\\\\frac{\\\\sqrt{\\\\left({\\\\frac{-b+{\\\\sqrt{b^{2}-4a c}}}{2a}}\\\\right)^{2}+4}}{2}}\\n\\\\end{equation}   \\n\\n\\n\\nEn resumen, los resultados sugieren que el programa de ejercicio físico implementado tuvo un impacto positivo en la reducción de los factores de riesgo cardiovascular y en la mejora de la capacidad física de los participantes.  \\n\\nEl chunking semántico es una técnica de procesamiento del lenguaje natural (PLN) que divide un texto en fragmentos (chunks) con significado independiente, como frases o cláusulas coherentes. A diferencia del chunking sintáctico, que separa palabras según su estructura gramatical (sujeto, verbo, objeto, etc.), el chunking semántico puede observarse en la Figura 2.',\n",
       "    'distance': 0.7421004772186279}]}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "all_results = []\n",
    "question = \"Qué es la inteligencia artificial?\"\n",
    "\n",
    "contexts, retrieval_time = retrieve_top_k(model, id_map, metadata, question)\n",
    "\n",
    "all_results.append({\n",
    "        \"question\": question,\n",
    "        \"contexts\": contexts,\n",
    "        #\"retrieval_time\": float(retrieval_time)  # Convert to regular float\n",
    "    })\n",
    "\n",
    "all_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
